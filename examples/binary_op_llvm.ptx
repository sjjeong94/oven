//
// Generated by LLVM NVPTX Back-End
//

.version 7.1
.target sm_86
.address_size 64

	// .globl	add

.visible .entry add(
	.param .u64 .ptr .align 1 add_param_0,
	.param .u64 .ptr .align 1 add_param_1,
	.param .u64 .ptr .align 1 add_param_2
)
{
	.reg .b32 	%r<5>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd1, [add_param_0];
	ld.param.u64 	%rd2, [add_param_2];
	cvta.to.global.u64 	%rd3, %rd2;
	ld.param.u64 	%rd4, [add_param_1];
	cvta.to.global.u64 	%rd5, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	mul.wide.s32 	%rd7, %r4, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f32 	%f1, [%rd8];
	add.s64 	%rd9, %rd5, %rd7;
	ld.global.f32 	%f2, [%rd9];
	add.rn.f32 	%f3, %f1, %f2;
	add.s64 	%rd10, %rd3, %rd7;
	st.global.f32 	[%rd10], %f3;
	ret;

}
	// .globl	mul
.visible .entry mul(
	.param .u64 .ptr .align 1 mul_param_0,
	.param .u64 .ptr .align 1 mul_param_1,
	.param .u64 .ptr .align 1 mul_param_2
)
{
	.reg .b32 	%r<5>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd1, [mul_param_0];
	ld.param.u64 	%rd2, [mul_param_2];
	cvta.to.global.u64 	%rd3, %rd2;
	ld.param.u64 	%rd4, [mul_param_1];
	cvta.to.global.u64 	%rd5, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	mul.wide.s32 	%rd7, %r4, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f32 	%f1, [%rd8];
	add.s64 	%rd9, %rd5, %rd7;
	ld.global.f32 	%f2, [%rd9];
	mul.rn.f32 	%f3, %f1, %f2;
	add.s64 	%rd10, %rd3, %rd7;
	st.global.f32 	[%rd10], %f3;
	ret;

}
	// .globl	sub
.visible .entry sub(
	.param .u64 .ptr .align 1 sub_param_0,
	.param .u64 .ptr .align 1 sub_param_1,
	.param .u64 .ptr .align 1 sub_param_2
)
{
	.reg .b32 	%r<5>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd1, [sub_param_0];
	ld.param.u64 	%rd2, [sub_param_2];
	cvta.to.global.u64 	%rd3, %rd2;
	ld.param.u64 	%rd4, [sub_param_1];
	cvta.to.global.u64 	%rd5, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	mul.wide.s32 	%rd7, %r4, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f32 	%f1, [%rd8];
	add.s64 	%rd9, %rd5, %rd7;
	ld.global.f32 	%f2, [%rd9];
	sub.rn.f32 	%f3, %f1, %f2;
	add.s64 	%rd10, %rd3, %rd7;
	st.global.f32 	[%rd10], %f3;
	ret;

}
	// .globl	div
.visible .entry div(
	.param .u64 .ptr .align 1 div_param_0,
	.param .u64 .ptr .align 1 div_param_1,
	.param .u64 .ptr .align 1 div_param_2
)
{
	.reg .b32 	%r<5>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd1, [div_param_0];
	ld.param.u64 	%rd2, [div_param_2];
	cvta.to.global.u64 	%rd3, %rd2;
	ld.param.u64 	%rd4, [div_param_1];
	cvta.to.global.u64 	%rd5, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	mul.wide.s32 	%rd7, %r4, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f32 	%f1, [%rd8];
	add.s64 	%rd9, %rd5, %rd7;
	ld.global.f32 	%f2, [%rd9];
	div.rn.f32 	%f3, %f1, %f2;
	add.s64 	%rd10, %rd3, %rd7;
	st.global.f32 	[%rd10], %f3;
	ret;

}
	// .globl	vadd
.visible .entry vadd(
	.param .u64 .ptr .align 1 vadd_param_0,
	.param .u64 .ptr .align 1 vadd_param_1,
	.param .u64 .ptr .align 1 vadd_param_2
)
{
	.reg .b32 	%r<6>;
	.reg .f32 	%f<13>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd1, [vadd_param_0];
	ld.param.u64 	%rd2, [vadd_param_2];
	cvta.to.global.u64 	%rd3, %rd2;
	ld.param.u64 	%rd4, [vadd_param_1];
	cvta.to.global.u64 	%rd5, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	shl.b32 	%r5, %r4, 2;
	mul.wide.s32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd8];
	add.s64 	%rd9, %rd5, %rd7;
	ld.global.v4.f32 	{%f5, %f6, %f7, %f8}, [%rd9];
	add.rn.f32 	%f9, %f4, %f8;
	add.rn.f32 	%f10, %f3, %f7;
	add.rn.f32 	%f11, %f2, %f6;
	add.rn.f32 	%f12, %f1, %f5;
	add.s64 	%rd10, %rd3, %rd7;
	st.global.v4.f32 	[%rd10], {%f12, %f11, %f10, %f9};
	ret;

}
	// .globl	vmul
.visible .entry vmul(
	.param .u64 .ptr .align 1 vmul_param_0,
	.param .u64 .ptr .align 1 vmul_param_1,
	.param .u64 .ptr .align 1 vmul_param_2
)
{
	.reg .b32 	%r<6>;
	.reg .f32 	%f<13>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd1, [vmul_param_0];
	ld.param.u64 	%rd2, [vmul_param_2];
	cvta.to.global.u64 	%rd3, %rd2;
	ld.param.u64 	%rd4, [vmul_param_1];
	cvta.to.global.u64 	%rd5, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	shl.b32 	%r5, %r4, 2;
	mul.wide.s32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd8];
	add.s64 	%rd9, %rd5, %rd7;
	ld.global.v4.f32 	{%f5, %f6, %f7, %f8}, [%rd9];
	mul.rn.f32 	%f9, %f4, %f8;
	mul.rn.f32 	%f10, %f3, %f7;
	mul.rn.f32 	%f11, %f2, %f6;
	mul.rn.f32 	%f12, %f1, %f5;
	add.s64 	%rd10, %rd3, %rd7;
	st.global.v4.f32 	[%rd10], {%f12, %f11, %f10, %f9};
	ret;

}
	// .globl	vsub
.visible .entry vsub(
	.param .u64 .ptr .align 1 vsub_param_0,
	.param .u64 .ptr .align 1 vsub_param_1,
	.param .u64 .ptr .align 1 vsub_param_2
)
{
	.reg .b32 	%r<6>;
	.reg .f32 	%f<13>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd1, [vsub_param_0];
	ld.param.u64 	%rd2, [vsub_param_2];
	cvta.to.global.u64 	%rd3, %rd2;
	ld.param.u64 	%rd4, [vsub_param_1];
	cvta.to.global.u64 	%rd5, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	shl.b32 	%r5, %r4, 2;
	mul.wide.s32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd8];
	add.s64 	%rd9, %rd5, %rd7;
	ld.global.v4.f32 	{%f5, %f6, %f7, %f8}, [%rd9];
	sub.rn.f32 	%f9, %f4, %f8;
	sub.rn.f32 	%f10, %f3, %f7;
	sub.rn.f32 	%f11, %f2, %f6;
	sub.rn.f32 	%f12, %f1, %f5;
	add.s64 	%rd10, %rd3, %rd7;
	st.global.v4.f32 	[%rd10], {%f12, %f11, %f10, %f9};
	ret;

}
	// .globl	vdiv
.visible .entry vdiv(
	.param .u64 .ptr .align 1 vdiv_param_0,
	.param .u64 .ptr .align 1 vdiv_param_1,
	.param .u64 .ptr .align 1 vdiv_param_2
)
{
	.reg .b32 	%r<6>;
	.reg .f32 	%f<13>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd1, [vdiv_param_0];
	ld.param.u64 	%rd2, [vdiv_param_2];
	cvta.to.global.u64 	%rd3, %rd2;
	ld.param.u64 	%rd4, [vdiv_param_1];
	cvta.to.global.u64 	%rd5, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	shl.b32 	%r5, %r4, 2;
	mul.wide.s32 	%rd7, %r5, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd8];
	add.s64 	%rd9, %rd5, %rd7;
	ld.global.v4.f32 	{%f5, %f6, %f7, %f8}, [%rd9];
	div.rn.f32 	%f9, %f4, %f8;
	div.rn.f32 	%f10, %f3, %f7;
	div.rn.f32 	%f11, %f2, %f6;
	div.rn.f32 	%f12, %f1, %f5;
	add.s64 	%rd10, %rd3, %rd7;
	st.global.v4.f32 	[%rd10], {%f12, %f11, %f10, %f9};
	ret;

}
